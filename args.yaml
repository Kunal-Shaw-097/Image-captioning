epochs: 5
initial_learning_rate: 1e-3

emb_dim: 512                                        #embedding dimension of tokens also output channels of encoder

num_heads: 8                                        #number of self attention heads in each transformer layer
num_layers: 6                                       #number of transformers layer in decoder

num_channel: 3                                      #number of input channels(3 for RGB images) to the encoder
channel_ratio: [1, 2, 5]                           #must add to 8
